{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2624724,"sourceType":"datasetVersion","datasetId":1595713},{"sourceId":6647464,"sourceType":"datasetVersion","datasetId":2093102}],"dockerImageVersionId":30260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2><center> <span style = \"font-family: Babas; font-size: 2em;\"> E-commerce Text Classification </span> </center></h2>\n<h4><center> <span style = \"font-family: Babas; font-size: 2em; font-style: italic\"> With TF-IDF and Word2Vec </span> </center></h4>\n","metadata":{}},{"cell_type":"markdown","source":"---\n\n### Overview\n\nThe objective of the project is to classify [**e-commerce**](https://en.wikipedia.org/wiki/E-commerce) products into four categories, based on its description available in the e-commerce platforms. The categories are: `Electronics`, `Household`, `Books`, and `Clothing & Accessories`. We carried out the following steps in this notebook:\n\n- Performed basic [**exploratory data analysis**](https://en.wikipedia.org/wiki/Exploratory_data_analysis), comparing the distributions of the number of characters, number of words, and average word-length of descriptions of products from different categories.\n- Employed several [**text normalization**](https://en.wikipedia.org/wiki/Text_normalization) techniques on product descriptions.\n- Used [**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) vectorizer on the normalized product descriptions for **text vectorization**, compared the baseline performance of several classifiers, and performed [**hyperparameter tuning**](https://en.wikipedia.org/wiki/Hyperparameter_optimization) on the [**support vector machine**](https://en.wikipedia.org/wiki/Support-vector_machine) classifier with **linear kernel**.\n- In a separate direction, employed a few selected **text normalization** processes, namely **convertion to lowercase** and **substitution of [contractions](https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions)** on the raw data on product descriptions; used [**Google**](https://en.wikipedia.org/wiki/Google)'s pre-trained [**Word2Vec**](https://en.wikipedia.org/wiki/Word2vec) model on the tokens, obtained from the partially normalized descriptions, to get the [**embeddings**](https://en.wikipedia.org/wiki/Word_embedding), which are then converted to [**compressed sparse row**](https://en.wikipedia.org/wiki/Sparse_matrix) (CSR) format; compared the baseline performance of several classifiers, and performed [**hyperparameter tuning**](https://en.wikipedia.org/wiki/Hyperparameter_optimization) on the [**XGBoost**](https://en.wikipedia.org/wiki/XGBoost) classifier.\n- Employed the model with the highest validation accuracy to predict the labels of the test observations and obtained a test accuracy of $0.948939$.\n\n---","metadata":{}},{"cell_type":"markdown","source":"### Contents\n\n- [**Introduction**](#Introduction)\n    - [E-commerce Product Categorization](#E-commerce-Product-Categorization)\n    - [Text Classification](#Text-Classification)\n    - [Data](#Data)\n    - [Project Objective](#Project-Objective)\n- [**Exploratory Data Analysis**](#Exploratory-Data-Analysis)\n    - [Class Frequencies](#Class-Frequencies)\n    - [Number of Characters](#Number-of-Characters)\n    - [Number of Words](#Number-of-Words)\n    - [Average Word-length](#Average-Word-length)\n- [**Train-Validation-Test Split**](#Train-Validation-Test-Split)\n- [**Text Normalization**](#Text-Normalization)\n    - [Convertion to Lowercase](#Convertion-to-Lowercase)\n    - [Removal of Whitespaces](#Removal-of-Whitespaces)\n    - [Removal of Punctuations](#Removal-of-Punctuations)\n    - [Removal of Unicode Characters](#Removal-of-Unicode-Characters)\n    - [Substitution of Acronyms](#Substitution-of-Acronyms)\n    - [Substitution of Contractions](#Substitution-of-Contractions)\n    - [Removal of Stop Words](#Removal-of-Stop-Words)\n    - [Spelling Correction](#Spelling-Correction)\n    - [Stemming and Lemmatization](#Stemming-and-Lemmatization)\n    - [Discardment of Non-alphabetic Words](#Discardment-of-Non-alphabetic-Words)\n    - [Retainment of Relevant Parts of Speech](#Retainment-of-Relevant-Parts-of-Speech)\n    - [Removal of Additional Stop Words](#Removal-of-Additional-Stop-Words)\n    - [Integration of the Processes](#Integration-of-the-Processes)\n    - [Implementation on Product Description](#Implementation-on-Product-Description)\n- [**TF-IDF Model**](#TF-IDF-Model)\n    - [Text Vectorization](#Text-Vectorization)\n    - [TF-IDF Baseline Modeling](#TF-IDF-Baseline-Modeling)\n    - [TF-IDF Hyperparameter Tuning](#TF-IDF-Hyperparameter-Tuning)\n- [**Word2Vec Model**](#Word2Vec-Model)\n    - [Partial Text Normalization](#Partial-Text-Normalization)\n    - [Word Embedding](#Word-Embedding)\n    - [Word2Vec Baseline Modeling](#Word2Vec-Baseline-Modeling)\n    - [Word2Vec Hyperparameter Tuning](#Word2Vec-Hyperparameter-Tuning)\n- [**Final Prediction and Evaluation**](#Final-Prediction-and-Evaluation)\n- [**Acknowledgements**](#Acknowledgements)\n- [**References**](#References)","metadata":{}},{"cell_type":"markdown","source":"### Importing libraries","metadata":{}},{"cell_type":"code","source":"# File system manangement\nimport time, psutil, os\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Plotting and visualization\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nsns.set_theme()\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# NLP\nimport string, re, nltk\nfrom string import punctuation\nfrom nltk.tokenize import word_tokenize, RegexpTokenizer\nfrom nltk.corpus import stopwords\n!pip install num2words\nfrom num2words import num2words\n!pip install pyspellchecker\nfrom spellchecker import SpellChecker\nfrom nltk.stem.porter import PorterStemmer\nimport spacy\nfrom nltk.stem import WordNetLemmatizer\n\n# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Scipy\nimport scipy\nfrom scipy import sparse\nfrom scipy.sparse import csr_matrix\n\n# Train-test split and cross validation\nfrom sklearn.model_selection import train_test_split, ParameterGrid\n\n# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Model evaluation\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\n# Others\nimport json\nimport gensim\nfrom sklearn.decomposition import TruncatedSVD","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-10-09T04:12:50.482238Z","iopub.execute_input":"2022-10-09T04:12:50.482787Z","iopub.status.idle":"2022-10-09T04:13:29.988883Z","shell.execute_reply.started":"2022-10-09T04:12:50.482675Z","shell.execute_reply":"2022-10-09T04:13:29.987466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Runtime and memory usage","metadata":{}},{"cell_type":"code","source":"# Recording the starting time, complemented with a stopping time check in the end to compute process runtime\nstart = time.time()\n\n# Class representing the OS process and having memory_info() method to compute process memory usage\nprocess = psutil.Process(os.getpid())","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-10-09T04:13:29.991434Z","iopub.execute_input":"2022-10-09T04:13:29.993165Z","iopub.status.idle":"2022-10-09T04:13:29.999861Z","shell.execute_reply.started":"2022-10-09T04:13:29.99311Z","shell.execute_reply":"2022-10-09T04:13:29.998589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\n- [**E-commerce Product Categorization**](#E-commerce-Product-Categorization)\n- [**Text Classification**](#Text-Classification)\n- [**Data**](#Data)\n- [**Project Objective**](#Project-Objective)","metadata":{}},{"cell_type":"markdown","source":"## E-commerce Product Categorization","metadata":{}},{"cell_type":"markdown","source":"**E-commerce** (electronic commerce) is the activity of electronically buying or selling of products on online services or over the [**Internet**](https://en.wikipedia.org/wiki/Internet). It frequently uses the [**web**](https://en.wikipedia.org/wiki/World_Wide_Web), if not entirely, for at least some of the life cycle of a transaction, while it may also use other technologies like [**e-mail**](https://en.wikipedia.org/wiki/Email). Typical e-commerce transactions include the purchase of products (like books from Amazon) or services (such as music downloads in the form of digital distribution such as the iTunes Store). [**Online retail**](https://en.wikipedia.org/wiki/Online_shopping), [**electronic markets**](https://en.wikipedia.org/wiki/Electronic_markets), and [**online auctions**](https://en.wikipedia.org/wiki/Online_auction) are the three areas of e-commerce. E-commerce is supported by [**electronic business**](https://en.wikipedia.org/wiki/Electronic_business). The purpose of e-commerce is to enable customers to shop and pay online over the Internet, saving both time and space for consumers and businesses while also significantly improving transaction efficiency.\n\n**Product categorization** or [**product classification**](https://en.wikipedia.org/wiki/Product_classification) is a type of [**economic taxonomy**](https://en.wikipedia.org/wiki/Economic_taxonomy) that refers to a system of categories into which a collection of products would fall under. Product categorization involves two separate tasks:\n\n- Create, support, and expand the catalogue structure for the offered products.\n- Tagging products with the correct categories and attributes.\n\nWhile machine learning does not have much potential for use in the first task, it is possible to automate the second task, which is relatively laborious and time-consuming. **The problem considered in this notebook involves the categorization of products offered on e-commerce platforms based on the descriptions of the products mentioned therein.** The purpose of such categorization is to enhance the user experience and achieve better results with external search engines. Visitors can quickly find the products they need by navigating the catalogue or using the website's search engine.","metadata":{}},{"cell_type":"markdown","source":"## Text Classification","metadata":{}},{"cell_type":"markdown","source":"**Text classification** is a widely used [**natural language processing**](https://en.wikipedia.org/wiki/Natural_language_processing) task in different business problems. Given a statement or document, the task involves assigning to it an appropriate category from a pre-defined set of categories. The dataset of choice determines the set of categories. Text classification has applications in emotion classification, news classification, spam email detection, auto-tagging of customer queries etc.\n\nIn the present problem, the statements are the product descriptions and the categories are `Electronics`, `Household`, `Books` and `Clothing & Accessories`.","metadata":{}},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"markdown","source":"**Original source:** **https://doi.org/10.5281/zenodo.3355823**\n\n**Kaggle dataset:** **https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification**\n\nThe dataset has been scraped from Indian e-commerce platform(s). It contains e-commerce text data for four categories: `Electronics`, `Household`, `Books` and `Clothing & Accessories`. Roughly speaking, these four categories cover $80\\%$ of any e-commerce website, by and large. The dataset is in *.csv* format and consists of two columns. The first column gives the target class name and the second column gives the datapoint, which is the description of the product from the e-commerce website. We insert column names and swap the columns, to put the target column at the right.","metadata":{}},{"cell_type":"code","source":"# Loading and customizing the data\ndata = pd.read_csv(\n    'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/Dataset/ecommerceDataset.csv', \n    names = ['label', 'description']\n)\ndata = data[['description', 'label']]\n\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(data.memory_usage().sum()/(1024*1024)), \n                 \"Dataset shape\": \"{}\".format(data.shape)}).to_string())\ndata","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:30.00158Z","iopub.execute_input":"2022-10-09T04:13:30.002453Z","iopub.status.idle":"2022-10-09T04:13:31.27036Z","shell.execute_reply.started":"2022-10-09T04:13:30.00241Z","shell.execute_reply":"2022-10-09T04:13:31.269241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example description\ndata['description'].iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:31.272913Z","iopub.execute_input":"2022-10-09T04:13:31.273411Z","iopub.status.idle":"2022-10-09T04:13:31.282631Z","shell.execute_reply.started":"2022-10-09T04:13:31.273377Z","shell.execute_reply":"2022-10-09T04:13:31.281598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values and duplicate observations\nprint(pd.Series({\"Number of observations with missing values\": len(data) - len(data.dropna()),\n                 \"Number of duplicate observations\": data.duplicated().sum()}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:31.283708Z","iopub.execute_input":"2022-10-09T04:13:31.28412Z","iopub.status.idle":"2022-10-09T04:13:31.342435Z","shell.execute_reply.started":"2022-10-09T04:13:31.284078Z","shell.execute_reply":"2022-10-09T04:13:31.340952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dropna(inplace = True) # Dropping observations with missing values\ndata.drop_duplicates(inplace = True) # Dropping duplicate observations\ndata.reset_index(drop = True, inplace = True) # Resetting index","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:31.343895Z","iopub.execute_input":"2022-10-09T04:13:31.344277Z","iopub.status.idle":"2022-10-09T04:13:31.469061Z","shell.execute_reply.started":"2022-10-09T04:13:31.344242Z","shell.execute_reply":"2022-10-09T04:13:31.467926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The labels are manually encoded with the following scheme:\n- **Electronics** $\\mapsto$ $0$\n- **Household** $\\mapsto$ $1$\n- **Books** $\\mapsto$ $2$\n- **Clothing & Accessories** $\\mapsto$ $3$","metadata":{}},{"cell_type":"code","source":"# Manual encoding of labels\nlabel_dict = {'Electronics': 0, 'Household': 1, 'Books': 2, 'Clothing & Accessories': 3}\ndata.replace({'label': label_dict}, inplace = True)\n\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(data.memory_usage().sum()/(1024*1024)),\n                 \"Dataset shape\": \"{}\".format(data.shape)}).to_string())\ndata","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:31.470763Z","iopub.execute_input":"2022-10-09T04:13:31.471541Z","iopub.status.idle":"2022-10-09T04:13:31.505761Z","shell.execute_reply.started":"2022-10-09T04:13:31.471494Z","shell.execute_reply":"2022-10-09T04:13:31.504357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Project Objective","metadata":{}},{"cell_type":"markdown","source":"The objective of the project is to classify a product into the four categories `Electronics`, `Household`, `Books` and `Clothing & Accessories`, based on its description available in the e-commerce platform.","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n- [**Class Frequencies**](#Class-Frequencies)\n- [**Number of Characters**](#Number-of-Characters)\n- [**Number of Words**](#Number-of-Words)\n- [**Average Word-length**](#Average-Word-length)","metadata":{}},{"cell_type":"markdown","source":"We split the dataset based on the target classes, in order to see how different textual attributes vary across classes.","metadata":{}},{"cell_type":"code","source":"# Splitting the dataset by label\ndata_e = data[data['label'] == 0] # Electronics\ndata_h = data[data['label'] == 1] # Household\ndata_b = data[data['label'] == 2] # Books\ndata_c = data[data['label'] == 3] # Clothing & Accessories","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:31.50743Z","iopub.execute_input":"2022-10-09T04:13:31.507891Z","iopub.status.idle":"2022-10-09T04:13:31.519972Z","shell.execute_reply.started":"2022-10-09T04:13:31.507851Z","shell.execute_reply":"2022-10-09T04:13:31.518254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class Frequencies","metadata":{}},{"cell_type":"code","source":"# Visualization of class frequencies\nvalues = np.array([len(data_e), len(data_h), len(data_b), len(data_c)])\nlabels = ['Electronics', 'Household', 'Books', 'Clothing & Accessories']\nfig = go.Figure(data = [go.Pie(values = values, labels = labels, hole = 0.5, textinfo = 'percent', title = \" \")])\ntext_title = \"Comparison of class frequencies\"\nfig.update_layout(height = 500, width = 800, showlegend = True, title = dict(text = text_title, x = 0.5, y = 0.95)) \nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:31.521803Z","iopub.execute_input":"2022-10-09T04:13:31.522487Z","iopub.status.idle":"2022-10-09T04:13:31.614243Z","shell.execute_reply.started":"2022-10-09T04:13:31.522448Z","shell.execute_reply":"2022-10-09T04:13:31.613004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of Characters","metadata":{}},{"cell_type":"code","source":"# Distribution of number of characters in description\ndata_e_char = data_e['description'].str.len()\ndata_h_char = data_h['description'].str.len()\ndata_b_char = data_b['description'].str.len()\ndata_c_char = data_c['description'].str.len()\n\nfig, ax = plt.subplots(2, 2, figsize = (10, 8.4), sharey = False)\nsns.histplot(x = data_e_char, bins = 20, ax = ax[0, 0]).set_title('Class: Electronics')\nsns.histplot(x = data_h_char, bins = 20, ax = ax[0, 1]).set_title('Class: Household')\nsns.histplot(x = data_b_char, bins = 20, ax = ax[1, 0]).set_title('Class: Books')\nsns.histplot(x = data_c_char, bins = 20, ax = ax[1, 1]).set_title('Class: Clothing & Accessories')\n\nfig.suptitle(\"Distribution of number of characters in description\")\nfor i in range(4):\n    ax[i // 2, i % 2].set_xlabel(\" \") if i // 2 == 0 else ax[i // 2, i % 2].set_xlabel(\"Number of characters\")\n    if i % 2 != 0: ax[i // 2, i % 2].set_ylabel(\" \")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:31.619762Z","iopub.execute_input":"2022-10-09T04:13:31.620296Z","iopub.status.idle":"2022-10-09T04:13:32.720623Z","shell.execute_reply.started":"2022-10-09T04:13:31.620247Z","shell.execute_reply":"2022-10-09T04:13:32.71978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of Words","metadata":{}},{"cell_type":"code","source":"# Distribution of number of words in description\ndata_e_word = data_e['description'].str.split().map(lambda x: len(x))\ndata_h_word = data_h['description'].str.split().map(lambda x: len(x))\ndata_b_word = data_b['description'].str.split().map(lambda x: len(x))\ndata_c_word = data_c['description'].str.split().map(lambda x: len(x))\n\nfig, ax = plt.subplots(2, 2, figsize = (10, 8.4), sharey = False)\nsns.histplot(x = data_e_word, bins = 20, ax = ax[0, 0]).set_title('Class: Electronics')\nsns.histplot(x = data_h_word, bins = 20, ax = ax[0, 1]).set_title('Class: Household')\nsns.histplot(x = data_b_word, bins = 20, ax = ax[1, 0]).set_title('Class: Books')\nsns.histplot(x = data_c_word, bins = 20, ax = ax[1, 1]).set_title('Class: Clothing & Accessories')\n\nfig.suptitle(\"Distribution of number of words in description\")\nfor i in range(4):\n    ax[i // 2, i % 2].set_xlabel(\" \") if i // 2 == 0 else ax[i // 2, i % 2].set_xlabel(\"Number of words\")\n    if i % 2 != 0: ax[i // 2, i % 2].set_ylabel(\" \")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:32.721934Z","iopub.execute_input":"2022-10-09T04:13:32.722453Z","iopub.status.idle":"2022-10-09T04:13:34.229725Z","shell.execute_reply.started":"2022-10-09T04:13:32.722417Z","shell.execute_reply":"2022-10-09T04:13:34.228423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Average Word-length","metadata":{}},{"cell_type":"code","source":"# Distribution of average word-length in description\ndata_e_avg = data_e['description'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\ndata_h_avg = data_h['description'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\ndata_b_avg = data_b['description'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\ndata_c_avg = data_c['description'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n\nfig, ax = plt.subplots(2, 2, figsize = (10, 8.4), sharey = False)\nsns.histplot(x = data_e_avg, bins = 20, ax = ax[0, 0]).set_title('Class: Electronics')\nsns.histplot(x = data_h_avg, bins = 20, ax = ax[0, 1]).set_title('Class: Household')\nsns.histplot(x = data_b_avg, bins = 20, ax = ax[1, 0]).set_title('Class: Books')\nsns.histplot(x = data_c_avg, bins = 20, ax = ax[1, 1]).set_title('Class: Clothing & Accessories')\n\nfig.suptitle(\"Distribution of average word-length in description\")\nfor i in range(4):\n    ax[i // 2, i % 2].set_xlabel(\" \") if i // 2 == 0 else ax[i // 2, i % 2].set_xlabel(\"Average word-length\")\n    if i % 2 != 0: ax[i // 2, i % 2].set_ylabel(\" \")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:34.231531Z","iopub.execute_input":"2022-10-09T04:13:34.231889Z","iopub.status.idle":"2022-10-09T04:13:36.900614Z","shell.execute_reply.started":"2022-10-09T04:13:34.231856Z","shell.execute_reply":"2022-10-09T04:13:36.899736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train-Validation-Test Split","metadata":{}},{"cell_type":"code","source":"# Feature-target split\nX, y = data.drop('label', axis = 1), data['label']\n\n# Train-test split (from complete data)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 40)\ndata_train = pd.concat([X_train, y_train], axis = 1)\n\n# Validation-test split (from test data)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 40)\ndata_val, data_test = pd.concat([X_val, y_val], axis = 1), pd.concat([X_test, y_test], axis = 1)\n\n# Comparison of sizes of training set, validation set and test set\nvalues = np.array([len(data_train), len(data_val), len(data_test)])\nlabels = ['Training set', 'Validation Set', 'Test set']\nfig = go.Figure(data = [go.Pie(values = values, labels = labels, hole = 0.5, textinfo = 'percent', title = \" \")])\ntext_title = \"Comparison of sizes of training set, validation set and test set\"\nfig.update_layout(height = 500, width = 800, showlegend = True, title = dict(text = text_title, x = 0.5, y = 0.95)) \nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:36.901924Z","iopub.execute_input":"2022-10-09T04:13:36.902499Z","iopub.status.idle":"2022-10-09T04:13:36.93879Z","shell.execute_reply.started":"2022-10-09T04:13:36.902463Z","shell.execute_reply":"2022-10-09T04:13:36.937486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Normalization\n\n- [**Convertion to Lowercase**](#Convertion-to-Lowercase)\n- [**Removal of Whitespaces**](#Removal-of-Whitespaces)\n- [**Removal of Punctuations**](#Removal-of-Punctuations)\n- [**Removal of Unicode Characters**](#Removal-of-Unicode-Characters)\n- [**Substitution of Acronyms**](#Substitution-of-Acronyms)\n- [**Substitution of Contractions**](#Substitution-of-Contractions)\n- [**Removal of Stop Words**](#Removal-of-Stop-Words)\n- [**Spelling Correction**](#Spelling-Correction)\n- [**Stemming and Lemmatization**](#Stemming-and-Lemmatization)\n- [**Discardment of Non-alphabetic Words**](#Discardment-of-Non-alphabetic-Words)\n- [**Retainment of Relevant Parts of Speech**](#Retainment-of-Relevant-Parts-of-Speech)\n- [**Removal of Additional Stop Words**](#Removal-of-Additional-Stop-Words)\n- [**Integration of the Processes**](#Integration-of-the-Processes)\n- [**Implementation on Product Description**](#Implementation-on-Product-Description)","metadata":{}},{"cell_type":"markdown","source":"In natural language processing, **text normalization** is the process of transforming text into a single canonical form. We consider a number of text normalization processes. At the end of the section, we combine selected processes into one single function and apply it on the product descriptions.","metadata":{}},{"cell_type":"code","source":"# RegexpTokenizer\nregexp = RegexpTokenizer(\"[\\w']+\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:36.94058Z","iopub.execute_input":"2022-10-09T04:13:36.941034Z","iopub.status.idle":"2022-10-09T04:13:36.947445Z","shell.execute_reply.started":"2022-10-09T04:13:36.940983Z","shell.execute_reply":"2022-10-09T04:13:36.946129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convertion to Lowercase","metadata":{}},{"cell_type":"markdown","source":"We convert all alphabetical characters of the tweets to lowercase so that the models do not differentiate identical words due to case-sensitivity. For example, without the normalization, *Sun* and *sun* would have been treated as two different words, which is not useful in the present context.","metadata":{}},{"cell_type":"code","source":"# Converting to lowercase\ndef convert_to_lowercase(text):\n    return text.lower()\n\ntext = \"This is a FUNCTION that CoNvErTs a Text to lowercase\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(convert_to_lowercase(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:36.949319Z","iopub.execute_input":"2022-10-09T04:13:36.950498Z","iopub.status.idle":"2022-10-09T04:13:36.960127Z","shell.execute_reply.started":"2022-10-09T04:13:36.95045Z","shell.execute_reply":"2022-10-09T04:13:36.958794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removal of Whitespaces","metadata":{}},{"cell_type":"markdown","source":"We remove the unnecessary empty spaces from the description of the observations.","metadata":{}},{"cell_type":"code","source":"# Removing whitespaces\ndef remove_whitespace(text):\n    return text.strip()\n\ntext = \" \\t This is a string \\t \"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_whitespace(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:36.961953Z","iopub.execute_input":"2022-10-09T04:13:36.963214Z","iopub.status.idle":"2022-10-09T04:13:36.974721Z","shell.execute_reply.started":"2022-10-09T04:13:36.963145Z","shell.execute_reply":"2022-10-09T04:13:36.9736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removal of Punctuations","metadata":{}},{"cell_type":"markdown","source":"Mostly the punctuations do not play any role in predicting whether a particular tweet indicate disaster or not. Thus we prevent them from contaminating the classification procedures by removing them from the tweets. However, we keep **apostrophe** since most of the contractions contain this punctuation and will be automatically taken care of once we convert the contractions.","metadata":{}},{"cell_type":"code","source":"# Removing punctuations\ndef remove_punctuation(text):\n    punct_str = string.punctuation\n    punct_str = punct_str.replace(\"'\", \"\") # discarding apostrophe from the string to keep the contractions intact\n    return text.translate(str.maketrans(\"\", \"\", punct_str))\n\ntext = \"Here's [an] example? {of} &a string. with.? punctuations!!!!\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_punctuation(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:36.976111Z","iopub.execute_input":"2022-10-09T04:13:36.976509Z","iopub.status.idle":"2022-10-09T04:13:36.985419Z","shell.execute_reply.started":"2022-10-09T04:13:36.976476Z","shell.execute_reply":"2022-10-09T04:13:36.984441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removal of Unicode Characters","metadata":{}},{"cell_type":"markdown","source":"The training tweets are typically sprinkled with emojis, URLs, punctuation and other symbols that do not contribute meaningfully to our analysis, but instead create noise in the learning procedure. Some of these symbols are unique, while the rest usually translate into unicode strings. We remove these irrelevant characters from the data using the regular expression module.","metadata":{}},{"cell_type":"code","source":"# Removing HTML tags\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\n\ntext = '<a href = \"https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification\"> Ecommerce Text Classification </a>'\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_html(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:36.98707Z","iopub.execute_input":"2022-10-09T04:13:36.987533Z","iopub.status.idle":"2022-10-09T04:13:36.999603Z","shell.execute_reply.started":"2022-10-09T04:13:36.9875Z","shell.execute_reply":"2022-10-09T04:13:36.99837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags = re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntext = \"This innovative hd printing technique results in durable and spectacular looking prints ðŸ˜Š\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_emoji(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.001355Z","iopub.execute_input":"2022-10-09T04:13:37.001815Z","iopub.status.idle":"2022-10-09T04:13:37.022787Z","shell.execute_reply.started":"2022-10-09T04:13:37.001773Z","shell.execute_reply":"2022-10-09T04:13:37.021565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing other unicode characters\ndef remove_http(text):\n    http = \"https?://\\S+|www\\.\\S+\" # matching strings beginning with http (but not just \"http\")\n    pattern = r\"({})\".format(http) # creating pattern\n    return re.sub(pattern, \"\", text)\n\ntext = \"It's a function that removes links starting with http: or https such as https://en.wikipedia.org/wiki/Unicode_symbols\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_http(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.023992Z","iopub.execute_input":"2022-10-09T04:13:37.024396Z","iopub.status.idle":"2022-10-09T04:13:37.032855Z","shell.execute_reply.started":"2022-10-09T04:13:37.024364Z","shell.execute_reply":"2022-10-09T04:13:37.03176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Substitution of Acronyms","metadata":{}},{"cell_type":"markdown","source":"[**Acronyms**](https://en.wikipedia.org/wiki/Acronym) are shortened forms of phrases, generally found in informal writings. For instance, *for your information* is written as *fyi* and *by the way* is written as *btw*. These time and effort-saving acronyms have received almost universal acceptance in recent times. For the sake of proper modeling, we convert the acronyms, appearing in the tweets, back to their respective original forms.","metadata":{}},{"cell_type":"code","source":"# Dictionary of acronyms\nacronyms_url = 'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_acronyms.json'\nacronyms_dict = pd.read_json(acronyms_url, typ = 'series')\n\nprint(\"Example: Original form of the acronym 'fyi' is '{}'\".format(acronyms_dict['fyi']))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.034381Z","iopub.execute_input":"2022-10-09T04:13:37.034786Z","iopub.status.idle":"2022-10-09T04:13:37.20559Z","shell.execute_reply.started":"2022-10-09T04:13:37.034754Z","shell.execute_reply":"2022-10-09T04:13:37.204402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataframe of acronyms\npd.DataFrame(acronyms_dict.items(), columns = ['acronym', 'original']).head()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.207114Z","iopub.execute_input":"2022-10-09T04:13:37.207623Z","iopub.status.idle":"2022-10-09T04:13:37.220937Z","shell.execute_reply.started":"2022-10-09T04:13:37.207576Z","shell.execute_reply":"2022-10-09T04:13:37.219278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of acronyms\nacronyms_list = list(acronyms_dict.keys())","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.222314Z","iopub.execute_input":"2022-10-09T04:13:37.222682Z","iopub.status.idle":"2022-10-09T04:13:37.231356Z","shell.execute_reply.started":"2022-10-09T04:13:37.222648Z","shell.execute_reply":"2022-10-09T04:13:37.230032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to convert contractions in a text\ndef convert_acronyms(text):\n    words = []\n    for word in regexp.tokenize(text):\n        if word in acronyms_list:\n            words = words + acronyms_dict[word].split()\n        else:\n            words = words + word.split()\n    \n    text_converted = \" \".join(words)\n    return text_converted\n\ntext = \"btw you've to fill in the details including dob\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(convert_acronyms(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.232709Z","iopub.execute_input":"2022-10-09T04:13:37.23303Z","iopub.status.idle":"2022-10-09T04:13:37.246157Z","shell.execute_reply.started":"2022-10-09T04:13:37.233002Z","shell.execute_reply":"2022-10-09T04:13:37.245262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Substitution of Contractions","metadata":{}},{"cell_type":"markdown","source":"A [**contraction**](https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions) is a shortened form of a word or a phrase, obtained by dropping one or more letters.\n\nThese are commonly used in everyday speech, written dialogue, informal writing and in situations where space is limited or costly, such as advertisements. Usually the missing letters are indicated by an apostrophe, but there are exceptions. Examples: **I'm = I am**, **let's = let us**, **won't = would not**, **howdy = how do you do**.\n\nWe have compiled an extensive list of English contractions, which can be found in the attached .json file titled *english_contractions*. The list is largely based on information obtained from [**this wikipedia page**](https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions) on *list of English contractions*. Note that the file only considers contractions in lowercase, i.e. it assumes that the textual data have already been transformed to lowercase before substituting the contractions. For example, the process will convert **i'll** to **i shall** but will leave **I'll** unchanged.","metadata":{}},{"cell_type":"code","source":"# Dictionary of contractions\ncontractions_url = 'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_contractions.json'\ncontractions_dict = pd.read_json(contractions_url, typ = 'series')\n\nprint(\"Example: Original form of the contraction 'aren't' is '{}'\".format(contractions_dict[\"aren't\"]))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.247722Z","iopub.execute_input":"2022-10-09T04:13:37.248027Z","iopub.status.idle":"2022-10-09T04:13:37.412001Z","shell.execute_reply.started":"2022-10-09T04:13:37.247999Z","shell.execute_reply":"2022-10-09T04:13:37.410883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataframe of contractions\npd.DataFrame(contractions_dict.items(), columns = ['contraction', 'original']).head()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.413351Z","iopub.execute_input":"2022-10-09T04:13:37.413729Z","iopub.status.idle":"2022-10-09T04:13:37.426399Z","shell.execute_reply.started":"2022-10-09T04:13:37.413694Z","shell.execute_reply":"2022-10-09T04:13:37.425228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The contractions do not always have a one-to-one mapping with the original words. For example **i'd** can come from both **i had** and **i would**. In the .json file only one the original words/phrases are chosen. However, this does not affect our analysis since words like **had** and **would**, which do not have any meaningful contribution in achieving the objective of the project, will be discarded in the next subsection.","metadata":{}},{"cell_type":"code","source":"# List of contractions\ncontractions_list = list(contractions_dict.keys())","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.427801Z","iopub.execute_input":"2022-10-09T04:13:37.428155Z","iopub.status.idle":"2022-10-09T04:13:37.437112Z","shell.execute_reply.started":"2022-10-09T04:13:37.428124Z","shell.execute_reply":"2022-10-09T04:13:37.436113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to convert contractions in a text\ndef convert_contractions(text):\n    words = []\n    for word in regexp.tokenize(text):\n        if word in contractions_list:\n            words = words + contractions_dict[word].split()\n        else:\n            words = words + word.split()\n    \n    text_converted = \" \".join(words)\n    return text_converted\n\ntext = \"he's doin' fine\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(convert_contractions(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.446385Z","iopub.execute_input":"2022-10-09T04:13:37.446779Z","iopub.status.idle":"2022-10-09T04:13:37.456497Z","shell.execute_reply.started":"2022-10-09T04:13:37.446748Z","shell.execute_reply":"2022-10-09T04:13:37.454978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removal of Stop Words","metadata":{}},{"cell_type":"markdown","source":"Several words, primarily pronouns, prepositions, modal verbs etc, are identified not to have much effect on the classification procedure. These are called [**stop words**](https://en.wikipedia.org/wiki/Stop_word). To get rid of the unwanted contamination effect, we remove these words.","metadata":{}},{"cell_type":"code","source":"# Stopwords\nstops = stopwords.words(\"english\") # stopwords\naddstops = [\"among\", \"onto\", \"shall\", \"thrice\", \"thus\", \"twice\", \"unto\", \"us\", \"would\"] # additional stopwords\nallstops = stops + addstops\n\nprint(allstops)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.458208Z","iopub.execute_input":"2022-10-09T04:13:37.458642Z","iopub.status.idle":"2022-10-09T04:13:37.476507Z","shell.execute_reply.started":"2022-10-09T04:13:37.458608Z","shell.execute_reply":"2022-10-09T04:13:37.4751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to remove stopwords from a list of texts\ndef remove_stopwords(text):\n    return \" \".join([word for word in regexp.tokenize(text) if word not in allstops])\n\ntext = \"This is a function that removes stopwords in a given text\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(remove_stopwords(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.478654Z","iopub.execute_input":"2022-10-09T04:13:37.47909Z","iopub.status.idle":"2022-10-09T04:13:37.487469Z","shell.execute_reply.started":"2022-10-09T04:13:37.479045Z","shell.execute_reply":"2022-10-09T04:13:37.486271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spelling Correction","metadata":{}},{"cell_type":"markdown","source":"The classification procedure cannot take mispellings into consideration and treats a word and its misspelt version as separate words. For this reason it is necessary to conduct spelling correction before feeding the data to the classification procedure.","metadata":{}},{"cell_type":"code","source":"# pyspellchecker\nspell = SpellChecker()\n\ndef pyspellchecker(text):\n    word_list = regexp.tokenize(text)\n    word_list_corrected = []\n    for word in word_list:\n        if word in spell.unknown(word_list):\n            word_corrected = spell.correction(word)\n            if word_corrected == None:\n                word_list_corrected.append(word)\n            else:\n                word_list_corrected.append(word_corrected)\n        else:\n            word_list_corrected.append(word)\n    text_corrected = \" \".join(word_list_corrected)\n    return text_corrected\n\ntext = \"I'm goinng therre\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(pyspellchecker(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.489462Z","iopub.execute_input":"2022-10-09T04:13:37.490265Z","iopub.status.idle":"2022-10-09T04:13:37.582529Z","shell.execute_reply.started":"2022-10-09T04:13:37.490218Z","shell.execute_reply":"2022-10-09T04:13:37.581597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stemming and Lemmatization","metadata":{}},{"cell_type":"markdown","source":"[**Stemming**](https://en.wikipedia.org/wiki/Stemming) is the process of reducing the words to their root form or **stem**. It reduces related words to the same *stem* even if the stem is not a dictionary word. For example, the words **introducing**, **introduced**, **introduction** reduce to a common word **introduce**. However, the process often produces stems that are not actual words.","metadata":{}},{"cell_type":"code","source":"# Stemming\nstemmer = PorterStemmer()\ndef text_stemmer(text):\n    text_stem = \" \".join([stemmer.stem(word) for word in regexp.tokenize(text)])\n    return text_stem\n\ntext = \"Introducing lemmatization as an improvement over stemming\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(text_stemmer(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.584032Z","iopub.execute_input":"2022-10-09T04:13:37.584607Z","iopub.status.idle":"2022-10-09T04:13:37.59248Z","shell.execute_reply.started":"2022-10-09T04:13:37.584567Z","shell.execute_reply":"2022-10-09T04:13:37.590856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The stems **introduc**, **lemmat** and **improv** are not actual words. [**Lemmatization**](https://en.wikipedia.org/wiki/Lemmatisation) offers a more sophisticated approach by utilizing a corpus to match root forms of the words. Unlike stemming, it uses the context in which a word is being used.","metadata":{}},{"cell_type":"code","source":"# Lemmatization\nspacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n#lemmatizer = WordNetLemmatizer()\n\ndef text_lemmatizer(text):\n    text_spacy = \" \".join([token.lemma_ for token in spacy_lemmatizer(text)])\n    #text_wordnet = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]) # regexp.tokenize(text)\n    return text_spacy\n    #return text_wordnet\n\ntext = \"Introducing lemmatization as an improvement over stemming\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(text_lemmatizer(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:37.593617Z","iopub.execute_input":"2022-10-09T04:13:37.593948Z","iopub.status.idle":"2022-10-09T04:13:38.499649Z","shell.execute_reply.started":"2022-10-09T04:13:37.593918Z","shell.execute_reply":"2022-10-09T04:13:38.498585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discardment of Non-alphabetic Words","metadata":{}},{"cell_type":"markdown","source":"The non-alphabetic words are not numerous and create unnecessary diversions in the context of classifying tweets into non-disaster and disaster categories. Hence we discard these words.","metadata":{}},{"cell_type":"code","source":"# Discardment of non-alphabetic words\ndef discard_non_alpha(text):\n    word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n    text_non_alpha = \" \".join(word_list_non_alpha)\n    return text_non_alpha\n\ntext = \"It is an ocean of thousands and 1000s of crowd\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(discard_non_alpha(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:38.500787Z","iopub.execute_input":"2022-10-09T04:13:38.501968Z","iopub.status.idle":"2022-10-09T04:13:38.509892Z","shell.execute_reply.started":"2022-10-09T04:13:38.501929Z","shell.execute_reply":"2022-10-09T04:13:38.508609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Retainment of Relevant Parts of Speech","metadata":{}},{"cell_type":"markdown","source":"The [**parts of speech**](https://en.wikipedia.org/wiki/Part_of_speech) provide a great tool to select a subset of words that are more likely to contribute in the classification procedure and discard the rest to avoid noise. The idea is to select a number of parts of speech that are important to the context of the problem. Then we partition the words in a given text into several subsets corresponding to each part of speech and keep only those subsets corresponding to the selected parts of speech. ","metadata":{}},{"cell_type":"code","source":"def keep_pos(text):\n    tokens = regexp.tokenize(text)\n    tokens_tagged = nltk.pos_tag(tokens)\n    #keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW']\n    keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW', 'PRP', 'PRPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WPS', 'WRB']\n    keep_words = [x[0] for x in tokens_tagged if x[1] in keep_tags]\n    return \" \".join(keep_words)\n\ntext = \"He arrived at seven o'clock on Wednesday evening\"\nprint(\"Input: {}\".format(text))\ntokens = regexp.tokenize(text)\nprint(\"Tokens: {}\".format(tokens))\ntokens_tagged = nltk.pos_tag(tokens)\nprint(\"Tagged Tokens: {}\".format(tokens_tagged))\nprint(\"Output: {}\".format(keep_pos(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:38.511515Z","iopub.execute_input":"2022-10-09T04:13:38.512346Z","iopub.status.idle":"2022-10-09T04:13:38.664127Z","shell.execute_reply.started":"2022-10-09T04:13:38.512295Z","shell.execute_reply":"2022-10-09T04:13:38.662843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For an extensive list of part-of-speech tags, see the [**alphabetical list of part-of-speech tags used in the Penn Treebank Project**](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).","metadata":{}},{"cell_type":"markdown","source":"## Removal of Additional Stop Words","metadata":{}},{"cell_type":"markdown","source":"Analyzing the data, we observe that several unnecessary words, which are not included in the ready-made set of **stop words**, keep appearing in the text corpus. We discard these words to remove noise in the classification procedure.","metadata":{}},{"cell_type":"code","source":"# Additional stopwords\n\nalphabets = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\nprepositions = [\"about\", \"above\", \"across\", \"after\", \"against\", \"among\", \"around\", \"at\", \"before\", \"behind\", \"below\", \"beside\", \"between\", \"by\", \"down\", \"during\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"near\", \"of\", \"off\", \"on\", \"out\", \"over\", \"through\", \"to\", \"toward\", \"under\", \"up\", \"with\"]\nprepositions_less_common = [\"aboard\", \"along\", \"amid\", \"as\", \"beneath\", \"beyond\", \"but\", \"concerning\", \"considering\", \"despite\", \"except\", \"following\", \"like\", \"minus\", \"onto\", \"outside\", \"per\", \"plus\", \"regarding\", \"round\", \"since\", \"than\", \"till\", \"underneath\", \"unlike\", \"until\", \"upon\", \"versus\", \"via\", \"within\", \"without\"]\ncoordinating_conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"and\", \"yet\"]\ncorrelative_conjunctions = [\"both\", \"and\", \"either\", \"or\", \"neither\", \"nor\", \"not\", \"only\", \"but\", \"whether\", \"or\"]\nsubordinating_conjunctions = [\"after\", \"although\", \"as\", \"as if\", \"as long as\", \"as much as\", \"as soon as\", \"as though\", \"because\", \"before\", \"by the time\", \"even if\", \"even though\", \"if\", \"in order that\", \"in case\", \"in the event that\", \"lest\", \"now that\", \"once\", \"only\", \"only if\", \"provided that\", \"since\", \"so\", \"supposing\", \"that\", \"than\", \"though\", \"till\", \"unless\", \"until\", \"when\", \"whenever\", \"where\", \"whereas\", \"wherever\", \"whether or not\", \"while\"]\nothers = [\"Ã£\", \"Ã¥\", \"Ã¬\", \"Ã»\", \"Ã»Âªm\", \"Ã»Ã³\", \"Ã»Ã²\", \"Ã¬Ã±\", \"Ã»Âªre\", \"Ã»Âªve\", \"Ã»Âª\", \"Ã»Âªs\", \"Ã»Ã³we\"]\nadditional_stops = alphabets + prepositions + prepositions_less_common + coordinating_conjunctions + correlative_conjunctions + subordinating_conjunctions + others\n\ndef remove_additional_stopwords(text):\n    return \" \".join([word for word in regexp.tokenize(text) if word not in additional_stops])","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:38.666349Z","iopub.execute_input":"2022-10-09T04:13:38.667217Z","iopub.status.idle":"2022-10-09T04:13:38.681122Z","shell.execute_reply.started":"2022-10-09T04:13:38.667153Z","shell.execute_reply":"2022-10-09T04:13:38.679623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Integration of the Processes","metadata":{}},{"cell_type":"markdown","source":"We integrate the text normalization processes in appropriate order. We have kept the **spelling correction** step commented out as it takes a massive amount of time to run on large datasets.","metadata":{}},{"cell_type":"code","source":"def text_normalizer(text):\n    text = convert_to_lowercase(text)\n    text = remove_whitespace(text)\n    text = re.sub('\\n' , '', text) # converting text to one line\n    text = re.sub('\\[.*?\\]', '', text) # removing square brackets\n    text = remove_http(text)\n    text = remove_punctuation(text)\n    text = remove_html(text)\n    text = remove_emoji(text)\n    text = convert_acronyms(text)\n    text = convert_contractions(text)\n    text = remove_stopwords(text)\n#     text = pyspellchecker(text)\n    text = text_lemmatizer(text) # text = text_stemmer(text)\n    text = discard_non_alpha(text)\n    text = keep_pos(text)\n    text = remove_additional_stopwords(text)\n    return text\n\ntext = \"We'll combine all functions into 1 SINGLE FUNCTION ðŸ™‚ & apply on @product #descriptions https://en.wikipedia.org/wiki/Text_normalization\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(text_normalizer(text)))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:38.683081Z","iopub.execute_input":"2022-10-09T04:13:38.683886Z","iopub.status.idle":"2022-10-09T04:13:38.70747Z","shell.execute_reply.started":"2022-10-09T04:13:38.683838Z","shell.execute_reply":"2022-10-09T04:13:38.70639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementation on Product Description","metadata":{}},{"cell_type":"code","source":"%%time\n# Implementing text normalization\ndata_train_norm, data_val_norm, data_test_norm = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\ndata_train_norm['normalized description'] = data_train['description'].apply(text_normalizer)\ndata_val_norm['normalized description'] = data_val['description'].apply(text_normalizer)\ndata_test_norm['normalized description'] = data_test['description'].apply(text_normalizer)\n\ndata_train_norm['label'] = data_train['label']\ndata_val_norm['label'] = data_val['label']\ndata_test_norm['label'] = data_test['label']\n\ndata_train_norm","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:13:38.708849Z","iopub.execute_input":"2022-10-09T04:13:38.709698Z","iopub.status.idle":"2022-10-09T04:20:44.18194Z","shell.execute_reply.started":"2022-10-09T04:13:38.70965Z","shell.execute_reply":"2022-10-09T04:20:44.180747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF-IDF Model\n\n- [**Text Vectorization**](#Text-Vectorization)\n- [**TF-IDF Baseline Modeling**](#TF-IDF-Baseline-Modeling)\n- [**TF-IDF Hyperparameter Tuning**](#TF-IDF-Hyperparameter-Tuning)","metadata":{}},{"cell_type":"markdown","source":"In the context of [**information retrieval**](https://en.wikipedia.org/wiki/Information_retrieval), **TF-IDF** (short for **term frequency-inverse document frequency**), is a numerical statistic that is intended to reflect how important a word is to a document in a collection or [**corpus**](https://en.wikipedia.org/wiki/Text_corpus). It is often used as a weighting factor in searches of **information retrieval**, [**text mining**](https://en.wikipedia.org/wiki/Text_mining), and [**user modeling**](https://en.wikipedia.org/wiki/User_modeling). The **TF-IDF** value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. The term is composed of two factors, **TF** and **IDF**.\n\n[**Term frequency**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency) **(TF)** is the relative frequency of a term within a given document. It is obtained as the number of times a word appears in a text, divided by the total number of words appearing in the text. Mathematically, it is given by\n\n$$ \\text{TF}\\left(t, d\\right) := \\frac{f_{t, d}}{\\sum_{t' \\in d} f_{t', d}},$$\n\nwhere $f_{t, d}$ is the number of times that term $t$ appears in the document $d$. Note that the denominator is simply the total number of terms in the document $d$, counting every instance of a given term.\n\n[**Inverse document frequency**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency) **(IDF)** measures how common or rare a word is across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that ratio. Mathematically, it is given by\n\n$$ \\text{IDF}\\left(t, D\\right) := \\log{\\frac{N}{\\left\\vert \\left\\{ d \\in D : t \\in d \\right\\} \\right\\vert}},$$\n\nwhere $\\left\\vert S \\right\\vert$ denotes the cardinality of the set $S$, $N$ is the total number of documents in the corpus, i.e. $N = \\left\\vert D \\right\\vert$, and the denominator $\\left\\vert \\left\\{ d \\in D : t \\in d \\right\\} \\right\\vert$ is the number of documents where the term $t$ appears. If $t$ is not in the corpus, then $\\text{IDF}\\left(t, D\\right)$ will become undefined. To avoid this, the denominator is often adjusted to $1 + \\left\\vert \\left\\{ d \\in D : t \\in d \\right\\} \\right\\vert$. **TF-IDF is the product of the two terms TF and IDF**, i.e.\n\n$$\\text{TF-IDF}\\left(t, d, D\\right) := \\text{TF}\\left(t, d\\right) \\times \\text{IDF}\\left(t, D\\right).$$\n\n**It objectively evaluates how relevant a word is to a text in a collection of texts, taking into consideration that some words appear more frequently in general.**","metadata":{}},{"cell_type":"markdown","source":"## Text Vectorization","metadata":{}},{"cell_type":"markdown","source":"In order to perform machine learning onÂ text data, we must transform the documents into vector representations. In [**natural language processing**](https://en.wikipedia.org/wiki/Natural_language_processing), **text vectorization**â€Šis the process ofâ€Šconverting words, sentences, or even larger units of text data to numerical vectors.","metadata":{}},{"cell_type":"code","source":"# Features and labels\nX_train_norm, y_train = data_train_norm['normalized description'].tolist(), data_train_norm['label'].tolist()\nX_val_norm, y_val = data_val_norm['normalized description'].tolist(), data_val_norm['label'].tolist()\nX_test_norm, y_test = data_test_norm['normalized description'].tolist(), data_test_norm['label'].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:20:44.183429Z","iopub.execute_input":"2022-10-09T04:20:44.183804Z","iopub.status.idle":"2022-10-09T04:20:44.194202Z","shell.execute_reply.started":"2022-10-09T04:20:44.18377Z","shell.execute_reply":"2022-10-09T04:20:44.192809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF-IDF vectorization\nTfidfVec = TfidfVectorizer(ngram_range = (1, 1))\nX_train_tfidf = TfidfVec.fit_transform(X_train_norm)\nX_val_tfidf = TfidfVec.transform(X_val_norm)\nX_test_tfidf = TfidfVec.transform(X_test_norm)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:20:44.195529Z","iopub.execute_input":"2022-10-09T04:20:44.195871Z","iopub.status.idle":"2022-10-09T04:20:45.715964Z","shell.execute_reply.started":"2022-10-09T04:20:44.19584Z","shell.execute_reply":"2022-10-09T04:20:45.714952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF Baseline Modeling","metadata":{}},{"cell_type":"code","source":"# Classifiers\nnames = [\n    \"Logistic Regression\",\n    \"KNN Classifier\",\n    \"Decision Tree\",\n    \"Linear SVM\",\n    \"Random Forest\",\n    \"SGD Classifier\",\n    \"Ridge Classifier\",\n    \"XGBoost\",\n    \"AdaBoost\",\n]\n\nmodels = [\n    LogisticRegression(max_iter = 1000),\n    KNeighborsClassifier(n_neighbors = 149, n_jobs = -1),\n    DecisionTreeClassifier(),\n    svm.SVC(kernel = 'linear'),\n    RandomForestClassifier(n_estimators = 100),\n    SGDClassifier(loss = 'hinge'),\n    RidgeClassifier(),\n    XGBClassifier(),\n    AdaBoostClassifier()\n]","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:20:45.717935Z","iopub.execute_input":"2022-10-09T04:20:45.718843Z","iopub.status.idle":"2022-10-09T04:20:45.727431Z","shell.execute_reply.started":"2022-10-09T04:20:45.718784Z","shell.execute_reply":"2022-10-09T04:20:45.726048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to return summary of baseline models\ndef score(X_train, y_train, X_val, y_val, names = names, models = models):\n    score_df, score_train, score_val = pd.DataFrame(), [], []\n    x = time.time()\n    for model in models:\n        model.fit(X_train, y_train)\n        y_train_pred, y_val_pred = model.predict(X_train), model.predict(X_val)\n        score_train.append(accuracy_score(y_train, y_train_pred))\n        score_val.append(accuracy_score(y_val, y_val_pred))\n    \n    score_df[\"Classifier\"], score_df[\"Training accuracy\"], score_df[\"Validation accuracy\"] = names, score_train, score_val\n    score_df.sort_values(by = 'Validation accuracy', ascending = False, inplace = True)\n    return score_df","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:20:45.729487Z","iopub.execute_input":"2022-10-09T04:20:45.72997Z","iopub.status.idle":"2022-10-09T04:20:45.743666Z","shell.execute_reply.started":"2022-10-09T04:20:45.729925Z","shell.execute_reply":"2022-10-09T04:20:45.742263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary of baseline models\nscore(X_train_tfidf, y_train, X_val_tfidf, y_val, names = names, models = models)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:20:45.745568Z","iopub.execute_input":"2022-10-09T04:20:45.746077Z","iopub.status.idle":"2022-10-09T04:26:28.138682Z","shell.execute_reply.started":"2022-10-09T04:20:45.746032Z","shell.execute_reply":"2022-10-09T04:26:28.136974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"We perform **hyperparameter tuning** on the best performing baseline model.","metadata":{}},{"cell_type":"code","source":"# Hyperparameter tuning for linear SVM\nsvm_classifier = svm.SVC()\nparams_svm = {\n    'kernel': ['linear'],\n    'C': [0.1, 1, 10, 100]\n}\n\nbest_model_svm, best_params_svm, best_score_svm, count = svm_classifier, ParameterGrid(params_svm)[0], 0, 0\nfor g in ParameterGrid(params_svm):\n    time_start = time.time()\n    count += 1\n    print(f\"Gridpoint #{count}: {g}\")\n    svm_classifier.set_params(**g)\n    svm_classifier.fit(X_train_tfidf, y_train)\n    y_train_pred, y_val_pred = svm_classifier.predict(X_train_tfidf), svm_classifier.predict(X_val_tfidf)\n    score_train, score_val = accuracy_score(y_train, y_train_pred), accuracy_score(y_val, y_val_pred)\n    time_stop = time.time()\n    m, s = int(time_stop - time_start) // 60, int(time_stop - time_start) % 60\n    print(f\"Training accuracy: {score_train}, Validation accuracy: {score_val}, Runtime: {m}m{s}s\")\n    print(\" \")\n    if score_val > best_score_svm:\n        best_params_svm, best_score_svm = g, score_val\n\nbest_model_tfidf, best_params_tfidf, best_score_tfidf = svm.SVC(), best_params_svm, best_score_svm\nbest_model_tfidf.set_params(**best_params_tfidf)\nprint(f\"Best model: {best_model_tfidf}\")\nprint(\" \")\nprint(f\"Best parameters: {best_params_tfidf}\")\nprint(f\"Best validation accuracy: {best_score_tfidf}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:26:28.14055Z","iopub.execute_input":"2022-10-09T04:26:28.140988Z","iopub.status.idle":"2022-10-09T04:38:03.574517Z","shell.execute_reply.started":"2022-10-09T04:26:28.140952Z","shell.execute_reply":"2022-10-09T04:38:03.573176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2Vec Model\n\n- [**Partial Text Normalization**](#Partial-Text-Normalization)\n- [**Word Embedding**](#Word-Embedding)\n- [**Word2Vec Baseline Modeling**](#Word2Vec-Baseline-Modeling)\n- [**Word2Vec Hyperparameter Tuning**](#Word2Vec-Hyperparameter-Tuning)","metadata":{}},{"cell_type":"markdown","source":"In the context of [**natural language processing**](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP), **word embeddings** are used for representing a word in terms of a real-valued vector that encodes the meaning of the word such that the words that are close in the vector space are expected to be similar in meaning. It can capture the context of a word in a document, as well as identify semantic and syntactic similarity and other contextual relations with other words in the document.\n\n**Word2Vec** is a specific word-embedding technique that uses a neural network model to learn word associations from a reasonably large corpus of text. After training, the model can detect similar words and recommend words to complete a partial sentence. As its name suggests, word2vec maps each distinct word to a vector, which is assigned in such a way that the level of semantic similarity between words is indicated by a simple mathematical operation on the vectors that the words are mapped to (for instance, the cosine similarity between the vectors).","metadata":{}},{"cell_type":"markdown","source":"## Partial Text Normalization","metadata":{}},{"cell_type":"markdown","source":"Standard text normalization processes like **stemming**, **lemmatization** or **removal of stop words** are not recommended when we have pre-trained embeddings. The reason behind this is that valuable information, which could be used by the neural network, is lost by those preprocessing steps. Here we shall consider a few selected text normalization processes only, before we feed the tokenized words to the pre-trained model to get the embeddings.","metadata":{}},{"cell_type":"code","source":"# Relevant text normalization processes\ndef convert_to_lowercase(text): return text.lower()\n\ncontractions_url = 'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_contractions.json'\ncontractions_dict = pd.read_json(contractions_url, typ = 'series')\ncontractions_list = list(contractions_dict.keys())\n\ndef convert_contractions(text):\n    words = []\n    for word in regexp.tokenize(text):\n        if word in contractions_list:\n            words = words + contractions_dict[word].split()\n        else:\n            words = words + word.split()\n    return \" \".join(words)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:38:03.576018Z","iopub.execute_input":"2022-10-09T04:38:03.576403Z","iopub.status.idle":"2022-10-09T04:38:03.720475Z","shell.execute_reply.started":"2022-10-09T04:38:03.57637Z","shell.execute_reply":"2022-10-09T04:38:03.719046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text normalization for Word2Vec\nfor df in [data_train, data_val, data_test]:\n    df['tokens'] = (df[\"description\"].apply(convert_to_lowercase)\n                                     .apply(convert_contractions)\n                                     .apply(regexp.tokenize))\ndata_train[['tokens', 'label']]","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:38:03.722052Z","iopub.execute_input":"2022-10-09T04:38:03.72267Z","iopub.status.idle":"2022-10-09T04:38:28.629165Z","shell.execute_reply.started":"2022-10-09T04:38:03.722631Z","shell.execute_reply":"2022-10-09T04:38:28.628253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Embedding","metadata":{}},{"cell_type":"code","source":"# Loading the pre-trained Word2Vec model\nword2vec_path = '../input/google-word2vec/GoogleNews-vectors-negative300.bin'\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary = True)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:38:28.630352Z","iopub.execute_input":"2022-10-09T04:38:28.631121Z","iopub.status.idle":"2022-10-09T04:39:38.281916Z","shell.execute_reply.started":"2022-10-09T04:38:28.631085Z","shell.execute_reply":"2022-10-09T04:39:38.280613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some useful functions for Word2Vec\ndef get_average_word2vec(tokens_list, vector, generate_missing = False, k = 300):\n    if len(tokens_list) < 1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n    length = len(vectorized)\n    summed = np.sum(vectorized, axis = 0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, tokens, generate_missing = False):\n    embeddings = tokens.apply(lambda x: get_average_word2vec(x, vectors, generate_missing = generate_missing))\n    return list(embeddings)\n\ndef plot_embedding(X, y):\n    truncated_SVD = TruncatedSVD(n_components = 2)\n    truncated_SVD.fit(X)\n    scores = truncated_SVD.transform(X)\n    color_mapper = {label:idx for idx, label in enumerate(set(y))}\n    color_column = [color_mapper[label] for label in y]\n    colors = ['red', 'blue', 'green', 'black']\n        \n    plt.scatter(scores[:, 0], scores[:, 1], s = 8, alpha = 0.8, c = y, cmap = matplotlib.colors.ListedColormap(colors))\n    red_patch = mpatches.Patch(color = 'red', label = 'Electronics')\n    blue_patch = mpatches.Patch(color = 'blue', label = 'Household')\n    green_patch = mpatches.Patch(color = 'green', label = 'Books')\n    black_patch = mpatches.Patch(color = 'black', label = 'Clothing & Accessories')\n    plt.legend(handles = [red_patch, blue_patch, green_patch, black_patch], prop = {\"size\": 12})","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:39:38.284439Z","iopub.execute_input":"2022-10-09T04:39:38.284947Z","iopub.status.idle":"2022-10-09T04:39:38.356967Z","shell.execute_reply.started":"2022-10-09T04:39:38.2849Z","shell.execute_reply":"2022-10-09T04:39:38.355975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word2Vec embedding\nX_train_embed = get_word2vec_embeddings(word2vec, data_train['tokens'])\nX_val_embed = get_word2vec_embeddings(word2vec, data_val['tokens'])\nX_test_embed = get_word2vec_embeddings(word2vec, data_test['tokens'])\n\nfig = plt.figure(figsize = (8, 7))          \nplot_embedding(X_train_embed, y_train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:39:38.358446Z","iopub.execute_input":"2022-10-09T04:39:38.359036Z","iopub.status.idle":"2022-10-09T04:39:52.969675Z","shell.execute_reply.started":"2022-10-09T04:39:38.359001Z","shell.execute_reply":"2022-10-09T04:39:52.968294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting to Compressed Sparse Row matrix\nX_train_w2v = scipy.sparse.csr_matrix(X_train_embed)\nX_val_w2v = scipy.sparse.csr_matrix(X_val_embed)\nX_test_w2v = scipy.sparse.csr_matrix(X_test_embed)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:39:52.970995Z","iopub.execute_input":"2022-10-09T04:39:52.971479Z","iopub.status.idle":"2022-10-09T04:39:53.415853Z","shell.execute_reply.started":"2022-10-09T04:39:52.971434Z","shell.execute_reply":"2022-10-09T04:39:53.414898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word2Vec Baseline Modeling","metadata":{}},{"cell_type":"code","source":"# Summary of baseline models\nscore(X_train_w2v, y_train, X_val_w2v, y_val, names = names, models = models)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T04:39:53.417382Z","iopub.execute_input":"2022-10-09T04:39:53.417818Z","iopub.status.idle":"2022-10-09T05:00:54.907021Z","shell.execute_reply.started":"2022-10-09T04:39:53.417785Z","shell.execute_reply":"2022-10-09T05:00:54.905853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word2Vec Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"We perform **hyperparameter tuning** on the best performing baseline model.","metadata":{}},{"cell_type":"code","source":"# Hyperparameter tuning for XGBoost\nxgb = XGBClassifier()\nparams_xgb = {\n    'learning_rate': [0.03, 0.3],\n    'min_child_weight': [0, 10],\n    'n_estimators': [200],\n    'reg_lambda': [1, 2],\n    'seed': [40]\n}\n\nbest_model_xgb, best_params_xgb, best_score_xgb, count = xgb, ParameterGrid(params_xgb)[0], 0, 0\nfor g in ParameterGrid(params_xgb):\n    time_start = time.time()\n    count += 1\n    print(f\"Gridpoint #{count}: {g}\")\n    xgb.set_params(**g)\n    xgb.fit(X_train_w2v, y_train)\n    y_train_pred, y_val_pred = xgb.predict(X_train_w2v), xgb.predict(X_val_w2v)\n    score_train, score_val = accuracy_score(y_train, y_train_pred), accuracy_score(y_val, y_val_pred)\n    time_stop = time.time()\n    m, s = int(time_stop - time_start) // 60, int(time_stop - time_start) % 60\n    print(f\"Training accuracy: {score_train}, Validation accuracy: {score_val}, Runtime: {m}m{s}s\")\n    print(\" \")\n    if score_val > best_score_xgb:\n        best_params_xgb, best_score_xgb = g, score_val\n\nbest_model_w2v, best_params_w2v, best_score_w2v = XGBClassifier(), best_params_xgb, best_score_xgb\nbest_model_w2v.set_params(**best_params_w2v)\nprint(f\"Best model: {best_model_w2v}\")\nprint(\" \")\nprint(f\"Best parameters: {best_params_w2v}\")\nprint(f\"Best validation accuracy: {best_score_w2v}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T05:00:54.908473Z","iopub.execute_input":"2022-10-09T05:00:54.909496Z","iopub.status.idle":"2022-10-09T07:29:34.136356Z","shell.execute_reply.started":"2022-10-09T05:00:54.909432Z","shell.execute_reply":"2022-10-09T07:29:34.135012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Prediction and Evaluation","metadata":{}},{"cell_type":"markdown","source":"We employ the model with the highest validation accuracy to predict the labels of the test observations and report the resulting test accuracy and confusion matrix.","metadata":{}},{"cell_type":"code","source":"# Function to compute and print confusion matrix\ndef conf_mat(y_test, y_test_pred, figsize = (10, 8), font_scale = 1.2, annot_kws_size = 16):\n    class_names = [0, 1, 2, 3] # ['Electronics', 'Household', 'Books', 'Clothing & Accessories']\n    tick_marks_y = [0.5, 1.5, 2.5, 3.5]\n    tick_marks_x = [0.5, 1.5, 2.5, 3.5]\n    confusion_matrix = metrics.confusion_matrix(y_test, y_test_pred)\n    confusion_matrix_df = pd.DataFrame(confusion_matrix, range(4), range(4))\n    plt.figure(figsize = figsize)\n    sns.set(font_scale = font_scale) # label size\n    plt.title(\"Confusion Matrix\")\n    sns.heatmap(confusion_matrix_df, annot = True, annot_kws = {\"size\": annot_kws_size}, fmt = 'd') # font size\n    plt.yticks(tick_marks_y, class_names, rotation = 'vertical')\n    plt.xticks(tick_marks_x, class_names, rotation = 'horizontal')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T07:29:34.138529Z","iopub.execute_input":"2022-10-09T07:29:34.140307Z","iopub.status.idle":"2022-10-09T07:29:34.15084Z","shell.execute_reply.started":"2022-10-09T07:29:34.140268Z","shell.execute_reply":"2022-10-09T07:29:34.149685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best model\nif best_score_tfidf >= best_score_w2v:\n    best_model, X_train_vec, X_test_vec = best_model_tfidf, X_train_tfidf, X_test_tfidf\nelse:\n    best_model, X_train_vec, X_test_vec = best_model_w2v, X_train_w2v, X_test_w2v","metadata":{"execution":{"iopub.status.busy":"2022-10-09T07:29:34.152626Z","iopub.execute_input":"2022-10-09T07:29:34.152964Z","iopub.status.idle":"2022-10-09T07:29:34.169728Z","shell.execute_reply.started":"2022-10-09T07:29:34.152933Z","shell.execute_reply":"2022-10-09T07:29:34.168673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction and evaluation on test set\nbest_model.fit(X_train_vec, y_train)\ny_test_pred = best_model.predict(X_test_vec)\nscore_test = accuracy_score(y_test, y_test_pred)\nprint(pd.Series({\"Test accuracy\": score_test}).to_string())\nprint(\" \")\nconf_mat(y_test, y_test_pred, figsize = (10, 8), font_scale = 1.2, annot_kws_size = 16) # Confusion matrix","metadata":{"execution":{"iopub.status.busy":"2022-10-09T07:29:34.171355Z","iopub.execute_input":"2022-10-09T07:29:34.171727Z","iopub.status.idle":"2022-10-09T07:30:54.914856Z","shell.execute_reply.started":"2022-10-09T07:29:34.171695Z","shell.execute_reply":"2022-10-09T07:30:54.913417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Runtime and memory usage\nstop = time.time()\nprint(pd.Series({\"Process runtime\": \"{:.2f} seconds\".format(float(stop - start)),\n                 \"Process memory usage\": \"{:.2f} MB\".format(float(process.memory_info()[0]/(1024*1024)))}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-10-09T07:30:54.916678Z","iopub.execute_input":"2022-10-09T07:30:54.917041Z","iopub.status.idle":"2022-10-09T07:30:54.928088Z","shell.execute_reply.started":"2022-10-09T07:30:54.917009Z","shell.execute_reply":"2022-10-09T07:30:54.926552Z"},"trusted":true},"execution_count":null,"outputs":[]}]}